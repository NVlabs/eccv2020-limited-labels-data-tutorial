<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="iccv, workshop, computer vision, computer graphics, visual learning, simulation environments, robotics, machine learning, natural language processing, reinforcement learning">

  <link rel="shortcut icon" href="/static/img/eccv-final.png">



  <title>New Frontiers for Learning with Limited Labels or Data</title>
  <meta name="description" content="Tutorial in Conjunction with ECCV 2020 ---">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Gaze Estimation and Prediction in the Wild"/>
  <meta property="og:url" content="https://gazeworkshop.github.io/"/>
  <meta property="og:description" content="Tutorial in Conjunction with ECCV 2020 ---"/>
  <meta property="og:site_name" content="Gaze Estimation and Prediction in the Wild"/>
  <meta property="og:image" content=""/>
  <meta property="og:image:url" content=""/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Gaze Estimation and Prediction in the Wild"/>
  <meta name="twitter:image" content="https://gazeworkshop.github.io/static/img/icon.jpg">
  <meta name="twitter:url" content="https://gazeworkshop.github.io"/>
  <meta name="twitter:description" content="Tutorial in Conjunction with ECCV 2020 ---"/>

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="/static/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="/static/css/main.css?1" media="screen,projection">

  <!-- Font Awesome -->
  <script src="https://kit.fontawesome.com/ff6e9b10da.js" crossorigin="anonymous"></script>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">

    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
	<li><a href="#intro">Introduction</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <li><a href="#organizers">Organizers</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>New Frontiers for Learning with Limited Labels or Data</h1></center>
    <center><h2>ECCV 2020 Tutorial, Glasgow, Scotland, UK</h2></center>
    <center>23-28 August 2020</center>
    <center>Location: <b>TDB</b></center>
  </div>
</div>

<div class="row" id="intro">
  <div class="col-md-12">
    <img src="/static/img/Glasgow.jpg" />
    <p align="right"> Photo: Pixabay</p>
  </div>
</div>

<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
Learning from limited data or supervision has garnered much research interest and many ideas to solve it are now well-established. Among them, learning from 1) partial or imperfect labels and 2) data from other related domains (synthetic data), modalities (text/audio/3D) or tasks have been extensively addressed in past tutorials/workshops. To improve network generalization when learning with limited data, the ideas of zero-, few-shot, meta- and task-transfer learning are ubiquitous. So, what's next? Are there new sub-problems that we have not yet investigated? Are there recent breakthroughs in other areas of vision and learning that can help us to better tackle this problem? Can we enable new applications?

<br />
<br />

To foster discussion and inspire new ideas along these lines, in this tutorial we will focus on new frontiers of learning with limited labels or data. We will explore emerging sub-problems, such as 1) omni-supervision, which is creating unified frameworks for learning from the entire spectrum of supervision -- with completely unsupervised, semi/weakly-supervised, to full-supervised data and 2) deep dreaming, which involves effectively inverting networks to generate training data. We will present some recent breakthroughs in other vision problems including, but not limited to, neural rendering; biologically-inspired self-learning and self-supervised image representation learning by exploiting video correspondences and 3D mesh reconstruction that can help to solve this problem. Finally, we will present some new applications including self-supervisedly learned discovery of visual object attributes.
      
<br id="schedule" />
<br />
<br />

<div class="row">
  <div class="col-xs-12">
     <h2>Schedule</h2>
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
	  <th>#</th>
		  <th>Time</th>
          <th>Item</th>
        </tr>
      </thead>
      <tbody>

        <tr>
	  <td> </td>
		  <td>8:15am - 8:20am</td>
          <td>Welcome and Opening Remarks, <a href="#shalini">Shalini De Mello</a> (NVIDIA)</td>
        </tr>

	<tr>
	  <td>1</td>
		  <td>8:20am - 9:00am</td>
          <td><b>Biologically Inspired Self-Learning</b>, <a href="#anima">Animashree Anandkumar</a> (NVIDIA, Caltech)</td>
        </tr>

	<tr>
	  <td>2</td>
		  <td>9:00am - 9:45am</td>
          <td><b>Inverting Neural Networks for Data-free Knowledge Transfer</b>, <a href="#pavlo">Pavlo Molchanov</a> (NVIDIA)</td>
        </tr>

        <tr>
	  <td> </td>
		  <td>9:45am - 10:00am</td>
          <td>Coffee Break</td>
        </tr>

        <tr>
	  <td>3</td>
		  <td>10:00am - 10:45am</td>
          <td><b>Self-supervised Learning for Video Correspondences and 3D Mesh Reconstruction</b>, <a href="#sifei">Sifei Liu</a> (NVIDIA)</td>
        </tr>

	<tr>
	  <td>4</td>
		  <td>10:45am - 11:30am</td>
          <td><b>Object Attribute Discovery from Image Collections</b>, <a href="#varun">Varun Jampani</a> (Google Research)</td>
        </tr>

	<tr>
	  <td>5</td>
		  <td>11:30am - 12:15pm</td>
          <td><b>Omni-supervised Learning: Unifying Levels of Supervision</b>, <a href="#zhiding">Zhiding Yu</a> (NVIDIA)</td>
        </tr>


        <tr id="organizers">
	  <td> </td>
		  <td>12:15pm - 12:20pm</td>
          <td> Closing Remarks, <a href="#shalini">Shalini De Mello</a> (NVIDIA)</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>


<br />


<div class="row">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row speaker" id="shalini">
  <div class="col-sm-3 speaker-pic">
    <a href="https://research.nvidia.com/person/shalini-gupta">
      <img class="people-pic" src="/static/img/people/sdm.jpg" />
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/person/shalini-gupta">Shalini De Mello</a>
      <h6>Nvidia</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
Shalini De Mello is a Principal Research Scientist at NVIDIA. Her research interests are in computer vision and machine learning for human-computer interaction and smart interfaces. At NVIDIA, she has invented technologies for gaze estimation, and 2D and 3D head pose estimation, hand gesture recognition, face detection, video stabilization and GPU-optimized libraries for mobile computer vision. Her research over that past several years has pushed the envelope of HCI in cars and has led to the development of NVIDIA’s innovative <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-ix/" target="_blank">DriveIX</a> product for smart <a href="https://www.youtube.com/watch?v=EVymqG9mdJg" target="_blank">AI-based automotive interfaces</a> for future generations of cars. She received doctoral and master’s degrees in Electrical and Computer Engineering from the University of Texas at Austin in 2008 and 2004, respectively.
    </p>
  </div>
</div>

<br />
<div class="row speaker" id="sifei">
  <div class="col-sm-3 speaker-pic">
    <a href="https://www.sifeiliu.net/">
      <img class="people-pic" src="/static/img/people/sl.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.sifeiliu.net/">Sifei Liu</a>
      <h6>Nvidia</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
Sifei Liu obtained her PhD at the University of California, Merced in EECS, advised by <a href="https://faculty.ucmerced.edu/mhyang"> Prof. Ming-Hsuan Yang</a>. Her research interests lie in computer vision, deep learning, and the combination of both. She has worked on self-supervised co-segmentation, video correspondence and 3D mesh reconstruction. She has co-organized the <a href="https://xiaolonw.github.io/graphnn/"> Learning Representations via Graph-structured Networks</a> tutorial in conjunction with CVPR 2019.
    </p>
  </div>
</div>

<br />
<div class="row speaker" id="zhiding">
  <div class="col-sm-3 speaker-pic">
    <a href="https://www.sifeiliu.net/">
      <img class="people-pic" src="/static/img/people/zy.jpg" />
    </a>
    <div class="people-name">
      <a href="https://chrisding.github.io/index.htm">Zhiding Yu</a>
      <h6>Nvidia</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
Zhiding Yu obtained his PhD in ECE from Carnegie Mellon University in 2017. His research interests include deep representation learning, weakly/semi-supervised learning, transfer learning and deep structured prediction, with their applications to vision problems. He has worked on noisy-label learning, self-training/semi-supervised learning for domain adaptation, uncertainty estimation, learning via data synthesis and weakly supervised learning. He is the winner of the Domain Adaptation for Semantic Segmentation Track, WAD Challenge at CVPR 2018.
    </p>
  </div>
</div>
<br />

<div class="row speaker" id="pavlo">
  <div class="col-sm-3 speaker-pic">
    <a href="https://research.nvidia.com/person/pavlo-molchanov">
      <img class="people-pic" src="/static/img/people/pm.jpg" />
    </a>
    <div class="people-name">
      <a href="https://research.nvidia.com/person/pavlo-molchanov">Pavlo Molchanov</a>
      <h6>Nvidia</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
Pavlo Molchanov obtained PhD from Tampere University of Technology, Finland in the area of signal processing in 2014. His dissertation was focused on designing automatic target recognition systems for radars. Since 2015 he is with the Learning and Perception Research team at NVIDIA, currently holding a senior research scientist position. His research is focused on methods for neural network acceleration, and designing novel systems for human-computer interaction and human understanding. For network acceleration, he is interested in neural network pruning methods and conditional inference. For human understanding, he is working on landmark estimation, gesture recognition, hand pose estimation. He received the EuRAD best paper award in 2011 and EuRAD young engineer award in 2013.
    </p>
  </div>
</div>
<br />

<div class="row speaker" id="varun">
  <div class="col-sm-3 speaker-pic">
    <a href="https://varunjampani.github.io/">
      <img class="people-pic" src="/static/img/people/vj.jpg" />
    </a>
    <div class="people-name">
      <a href="https://varunjampani.github.io/">Varun Jampani</a>
      <h6>Google Research</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
Varun Jampani is a Research Scientist at Google Research in Cambridge, US. Prior to that, he was a Research Scientist at NVIDIA Research. He works in the areas of machine learning and computer vision and his main research interests include content-adaptive neural networks, self-supervised visual discovery, motion understanding and novel view synthesis. He obtained his Ph.D. with highest honors at Max Planck Institute for Intelligent Systems (MPI) in 2017. He obtained his BTech and MS from International Institute of Information Technology, Hyderabad (IIIT-H), India, where he was a gold medalist. His work on <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf"> “SplatNet”</a> received the Best Paper Honorable Mention award at CVPR 2018.
    </p>
  </div>
</div>
<br />

<div class="row speaker" id="anima">
  <div class="col-sm-3 speaker-pic">
    <a href="http://tensorlab.cms.caltech.edu/users/anima/">
      <img class="people-pic" src="/static/img/people/aa.jpg" />
    </a>
    <div class="people-name">
      <a href="http://tensorlab.cms.caltech.edu/users/anima/">Animashree Anandkumar</a>
      <h6>NVIDIA and California Institute of Technology</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
	TBD
    </p>
  </div>
</div>
<br />

<div class="row speaker" id="jan">
  <div class="col-sm-3 speaker-pic">
    <a href="http://jankautz.com/">
      <img class="people-pic" src="/static/img/people/jk.jpg" />
    </a>
    <div class="people-name">
      <a href="http://jankautz.com/">Jan Kautz</a>
      <h6>NVIDIA</h6>
    </div>
  </div>
  <div class="col-md-9">
    <b>Biography</b>
    <p class="speaker-bio">
	Jan Kautz is VP of Learning and Perception Research at NVIDIA. Jan and his team pursue fundamental research in the areas of computer vision and deep learning, including visual perception, geometric vision, generative models, and efficient deep learning. Before joining NVIDIA in 2013, Jan was a tenured faculty member at University College London. He holds a BSc in Computer Science from the University of Erlangen-Nürnberg (1999), an MMath from the University of Waterloo (1999), received his PhD from the Max-Planck-Institut für Informatik (2003), and worked as a post-doctoral researcher at the Massachusetts Institute of Technology (2003-2006).
    </p>
  </div>
</div>
<br />
</p></div></div>

      </div>
    </div>

                <div class="section text-gray" id="footer">
                <div class="container">

                    <div class="row">

                        <div class="col-sm-6">

                            <!-- <p class="social">
                                <a href="mailto:organizers@pirm2018.org" class="email" data-animate-hover="shake" data-animate="fadeInUp"><i class="fa fa-envelope"></i></a>
                            </p> -->
                        </div>
                        <!-- /.6 -->
                      
                        <div class="col-sm-6">
                            <p><small>&copy; 2019 <a href="http://horanyinora.github.io" class="external">Nora Horanyi</a>.
                            Template by <a href=" visualdialog.org" class="external"> visualdialog.org</a>.</small></p>
                        </div>

                    </div>

                </div>
            </div>


    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
  </body>
</html>